import { LLMConfig, LLMResponse, ChatMessage } from '../types/index';
import { GoogleGenerativeAI } from '@google/generative-ai';
import chalk from 'chalk';

export class LLMService {
  private config: LLMConfig;
  private geminiClient?: GoogleGenerativeAI;

  constructor(config: LLMConfig) {
    this.config = config;
    this.initializeClient();
  }

  private initializeClient(): void {
    switch (this.config.provider) {
      case 'gemini':
        if (!this.config.apiKey) {
          throw new Error('Google APIå¯†é’¥æœªè®¾ç½®ã€‚è¯·è®¾ç½®GOOGLE_API_KEYç¯å¢ƒå˜é‡ã€‚');
        }
        this.geminiClient = new GoogleGenerativeAI(this.config.apiKey);
        break;
      
      case 'mock':
        // æ¨¡æ‹Ÿæ¨¡å¼ï¼Œä¸éœ€è¦åˆå§‹åŒ–å®¢æˆ·ç«¯
        break;
      
      default:
        throw new Error(`ä¸æ”¯æŒçš„LLMæä¾›å•†: ${this.config.provider}`);
    }
  }

  public async generateResponse(messages: ChatMessage[], systemPrompt?: string): Promise<LLMResponse> {
    try {
      switch (this.config.provider) {
        case 'gemini':
          return await this.generateGeminiResponse(messages, systemPrompt);
        
        case 'mock':
          return await this.generateMockResponse(messages);
        
        default:
          throw new Error(`ä¸æ”¯æŒçš„LLMæä¾›å•†: ${this.config.provider}`);
      }
    } catch (error) {
      console.error(chalk.red('LLMè°ƒç”¨å¤±è´¥:'), error);
      throw error;
    }
  }

  private async generateGeminiResponse(messages: ChatMessage[], systemPrompt?: string): Promise<LLMResponse> {
    if (!this.geminiClient) {
      throw new Error('Geminiå®¢æˆ·ç«¯æœªåˆå§‹åŒ–');
    }

    const model = this.geminiClient.getGenerativeModel({ 
      model: this.config.model,
      generationConfig: {
        maxOutputTokens: this.config.maxTokens,
        temperature: this.config.temperature,
      }
    });

    // è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    const lastUserMessage = messages.filter(m => m.role === 'user').pop();
    if (!lastUserMessage) {
      throw new Error('æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯');
    }

    // æ„å»ºpromptï¼ŒåŠ ä¸Šç³»ç»Ÿæç¤ºå’ŒåŸºæœ¬çš„å¯¹è¯ä¸Šä¸‹æ–‡
    let prompt = '';
    
    if (systemPrompt) {
      prompt += `${systemPrompt}\n\n`;
    }
    
    // å¦‚æœæœ‰å†å²æ¶ˆæ¯ï¼ŒåªåŒ…å«æœ€è¿‘çš„å‡ æ¡
    const recentMessages = messages.slice(-4); // åªä¿ç•™æœ€è¿‘4æ¡æ¶ˆæ¯
    if (recentMessages.length > 1) {
      recentMessages.slice(0, -1).forEach(msg => {
        const role = msg.role === 'user' ? 'ç”¨æˆ·' : 'åŠ©æ‰‹';
        prompt += `${role}: ${msg.content}\n`;
      });
      prompt += '\n';
    }
    
    // æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    prompt += `ç”¨æˆ·: ${lastUserMessage.content}\n`;
    prompt += 'åŠ©æ‰‹: ';

    console.log(chalk.green('ğŸ¤– prompt------:'), prompt);

    const result = await model.generateContent(prompt);
    const response = await result.response;
    const content = response.text() || 'æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç”Ÿæˆå›å¤ã€‚';

    // Gemini APIç›®å‰ä¸æä¾›è¯¦ç»†çš„tokenä½¿ç”¨ä¿¡æ¯ï¼Œæˆ‘ä»¬è¿›è¡Œä¼°ç®—
    const estimatedPromptTokens = Math.floor(prompt.length / 4);
    const estimatedCompletionTokens = Math.floor(content.length / 4);

    return {
      content,
      usage: {
        promptTokens: estimatedPromptTokens,
        completionTokens: estimatedCompletionTokens,
        totalTokens: estimatedPromptTokens + estimatedCompletionTokens
      }
    };
  }

  private async generateMockResponse(messages: ChatMessage[]): Promise<LLMResponse> {
    // æ¨¡æ‹Ÿæ€è€ƒæ—¶é—´
    await new Promise(resolve => setTimeout(resolve, 800 + Math.random() * 1200));

    const lastMessage = messages[messages.length - 1];
    const input = lastMessage?.content.toLowerCase().trim() || '';

    // åŸºç¡€å¯¹è¯é€»è¾‘ï¼ˆä¿æŒåŸæœ‰çš„æ¨¡æ‹Ÿé€»è¾‘ï¼‰
    let content = '';

    if (input.includes('ä½ å¥½') || input.includes('hello') || input.includes('hi')) {
      content = 'ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸æ‚¨å¯¹è¯ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨è§£ç­”é—®é¢˜ã€è¿›è¡Œè®¨è®ºæˆ–æä¾›ä¿¡æ¯ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ';
    } else if (input.includes('ä½ æ˜¯è°') || input.includes('ä»‹ç»') || input.includes('about')) {
      content = 'æˆ‘æ˜¯ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„AIåŠ©æ‰‹ï¼Œå…·å¤‡è‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€è¿›è¡Œå¯¹è¯ã€æä¾›å»ºè®®ç­‰ã€‚æˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„ä¿¡æ¯ã€‚';
    } else if (input.includes('æ—¶é—´') || input.includes('ç°åœ¨å‡ ç‚¹')) {
      const now = new Date();
      content = `å½“å‰æ—¶é—´æ˜¯ï¼š${now.toLocaleString('zh-CN', { 
        year: 'numeric', 
        month: '2-digit', 
        day: '2-digit', 
        hour: '2-digit', 
        minute: '2-digit', 
        second: '2-digit' 
      })}`;
    } else {
      // ç”Ÿæˆæ›´æ™ºèƒ½çš„é»˜è®¤å›å¤
      const responses = [
        'è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„è¯é¢˜ã€‚èƒ½è¯¦ç»†è¯´è¯´æ‚¨çš„æƒ³æ³•å—ï¼Ÿ',
        'æˆ‘ç†è§£æ‚¨çš„è§‚ç‚¹ã€‚è¿™ä¸ªé—®é¢˜ç¡®å®å€¼å¾—æ·±å…¥æ€è€ƒã€‚',
        'æ„Ÿè°¢æ‚¨åˆ†äº«è¿™ä¸ªä¿¡æ¯ã€‚æ‚¨å¸Œæœ›æˆ‘ä»å“ªä¸ªè§’åº¦æ¥åˆ†æå‘¢ï¼Ÿ',
        'è¿™è®©æˆ‘æƒ³åˆ°äº†ç›¸å…³çš„ä¸€äº›æ¦‚å¿µã€‚æ‚¨æƒ³äº†è§£æ›´å¤šèƒŒæ™¯ä¿¡æ¯å—ï¼Ÿ',
        'å¾ˆå¥½çš„é—®é¢˜ï¼è®©æˆ‘ä¸ºæ‚¨è¯¦ç»†è§£é‡Šä¸€ä¸‹ã€‚',
        'æˆ‘ä¼šå°½åŠ›å¸®åŠ©æ‚¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¯·å‘Šè¯‰æˆ‘æ›´å¤šç»†èŠ‚ã€‚'
      ];
      content = responses[Math.floor(Math.random() * responses.length)] || 'æˆ‘ä¼šå°½åŠ›å¸®åŠ©æ‚¨è§£å†³è¿™ä¸ªé—®é¢˜ã€‚';
    }

    return {
      content,
      usage: {
        promptTokens: Math.floor(input.length / 4), // å¤§æ¦‚ä¼°ç®—
        completionTokens: Math.floor(content.length / 4),
        totalTokens: Math.floor((input.length + content.length) / 4)
      }
    };
  }

  public getConfig(): LLMConfig {
    return { ...this.config };
  }

  public updateConfig(newConfig: Partial<LLMConfig>): void {
    this.config = { ...this.config, ...newConfig };
    this.initializeClient();
  }
} 