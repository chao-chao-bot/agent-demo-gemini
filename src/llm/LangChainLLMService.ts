import { ChatGoogleGenerativeAI } from '@langchain/google-genai';
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { RunnableSequence } from '@langchain/core/runnables';
import { BaseMessage, HumanMessage, AIMessage, SystemMessage } from '@langchain/core/messages';
import { LLMConfig, LLMResponse, ChatMessage } from '../types/index.js';
import chalk from 'chalk';

export class LangChainLLMService {
  private config: LLMConfig;
  private model!: ChatGoogleGenerativeAI;
  private chain: RunnableSequence<any, string> | null = null;

  constructor(config: LLMConfig) {
    this.config = config;
    this.initializeModel();
  }

  private initializeModel(): void {
    if (this.config.provider === 'mock') {
      // Mockæ¨¡å¼ï¼šåˆ›å»ºç®€å•çš„æµ‹è¯•é“¾
      console.log(chalk.yellow('âš ï¸ ä½¿ç”¨Mockæ¨¡å¼ï¼ŒLangChainåŠŸèƒ½å°†è¢«æ¨¡æ‹Ÿ'));
      this.createMockChain();
      return;
    }

    if (this.config.provider !== 'gemini') {
      throw new Error(`LangChainæœåŠ¡ç›®å‰åªæ”¯æŒGeminiå’ŒMockæ¨¡å¼ï¼Œå½“å‰é…ç½®: ${this.config.provider}`);
    }

    if (!this.config.apiKey) {
      throw new Error('Google APIå¯†é’¥æœªè®¾ç½®ã€‚è¯·è®¾ç½®GEMINI_API_KEYç¯å¢ƒå˜é‡ã€‚');
    }

    // åˆå§‹åŒ–Geminiæ¨¡å‹
    this.model = new ChatGoogleGenerativeAI({
      apiKey: this.config.apiKey,
      model: this.config.model,
      temperature: this.config.temperature,
      maxOutputTokens: this.config.maxTokens,
    });

    // åˆ›å»ºå¯¹è¯é“¾
    this.createConversationChain();
  }

  private createConversationChain(): void {
    // åˆ›å»ºæç¤ºæ¨¡æ¿
    const prompt = ChatPromptTemplate.fromMessages([
      ["system", "{system_prompt}"],
      new MessagesPlaceholder("chat_history"),
      ["human", "{input}"]
    ]);

    // åˆ›å»ºè¾“å‡ºè§£æå™¨
    const outputParser = new StringOutputParser();

    // åˆ›å»ºå¯è¿è¡Œé“¾
    this.chain = RunnableSequence.from([
      prompt,
      this.model,
      outputParser
    ]);
  }

  private createMockChain(): void {
    // ä¸ºMockæ¨¡å¼åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿé“¾
    this.chain = {
      invoke: async (input: any) => {
        const { system_prompt, chat_history, input: userInput } = input;
        
        // æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await new Promise(resolve => setTimeout(resolve, 500));
        
        // ç®€å•çš„æ¨¡æ‹Ÿå“åº”
        const responses = [
          `åŸºäºLangChainæ¡†æ¶ï¼Œæˆ‘ç†è§£æ‚¨è¯¢é—®å…³äº"${userInput}"çš„é—®é¢˜ã€‚`,
          `è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„LangChainå“åº”ã€‚æ‚¨çš„é—®é¢˜æ˜¯ï¼š${userInput}`,
          `åœ¨Mockæ¨¡å¼ä¸‹ï¼Œæˆ‘ä¼šæ¨¡æ‹ŸLangChainçš„RAGåŠŸèƒ½æ¥å›ç­”æ‚¨å…³äº"${userInput}"çš„é—®é¢˜ã€‚`,
          `LangChainæ¨¡æ‹ŸæœåŠ¡æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚ï¼š"${userInput}"ã€‚è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”ã€‚`
        ];
        
        const randomResponse = responses[Math.floor(Math.random() * responses.length)];
        return randomResponse;
      }
    } as any;
  }

  public async generateResponse(messages: ChatMessage[], systemPrompt?: string): Promise<LLMResponse> {
    if (!this.chain) {
      throw new Error('LangChainé“¾æœªåˆå§‹åŒ–');
    }

    try {
      console.log(chalk.green('ğŸ¤– ä½¿ç”¨LangChainå¤„ç†å¯¹è¯...'));

      // è½¬æ¢æ¶ˆæ¯æ ¼å¼
      const chatHistory = this.convertToChatHistory(messages.slice(0, -1)); // é™¤äº†æœ€åä¸€æ¡æ¶ˆæ¯
      const lastMessage = messages[messages.length - 1];
      
      if (!lastMessage || lastMessage.role !== 'user') {
        throw new Error('æœ€åä¸€æ¡æ¶ˆæ¯å¿…é¡»æ˜¯ç”¨æˆ·æ¶ˆæ¯');
      }

      const input = {
        system_prompt: systemPrompt || 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚',
        chat_history: chatHistory,
        input: lastMessage.content
      };

      console.log(chalk.blue('ğŸ“¤ å‘é€åˆ°LangChain:'), {
        system_prompt: input.system_prompt.substring(0, 100) + '...',
        chat_history_length: chatHistory.length,
        input: input.input.substring(0, 100) + '...'
      });

      // è°ƒç”¨é“¾
      const response = await this.chain.invoke(input);

      console.log(chalk.green('ğŸ“¥ LangChainå“åº”:'), response.substring(0, 100) + '...');

      // ä¼°ç®—tokenä½¿ç”¨é‡
      const promptTokens = this.estimateTokens(JSON.stringify(input));
      const completionTokens = this.estimateTokens(response);

      return {
        content: response,
        usage: {
          promptTokens,
          completionTokens,
          totalTokens: promptTokens + completionTokens
        }
      };

    } catch (error) {
      console.error(chalk.red('âŒ LangChainè°ƒç”¨å¤±è´¥:'), error);
      throw error;
    }
  }

  private convertToChatHistory(messages: ChatMessage[]): BaseMessage[] {
    return messages.map(msg => {
      switch (msg.role) {
        case 'user':
          return new HumanMessage(msg.content);
        case 'assistant':
          return new AIMessage(msg.content);
        case 'system':
          return new SystemMessage(msg.content);
        default:
          return new HumanMessage(msg.content);
      }
    });
  }

  private estimateTokens(text: string): number {
    // ç®€å•çš„tokenä¼°ç®—ï¼š1ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
    return Math.floor(text.length / 4);
  }

  public getConfig(): LLMConfig {
    return { ...this.config };
  }

  public updateConfig(newConfig: Partial<LLMConfig>): void {
    this.config = { ...this.config, ...newConfig };
    this.initializeModel();
  }

  // åˆ›å»ºRAGå¢å¼ºçš„å¯¹è¯é“¾
  public async generateRAGResponse(
    query: string, 
    context: string, 
    chatHistory: ChatMessage[] = [],
    systemPrompt?: string
  ): Promise<LLMResponse> {
    if (!this.chain) {
      throw new Error('LangChainé“¾æœªåˆå§‹åŒ–');
    }

    try {
      console.log(chalk.green('ğŸ” ä½¿ç”¨LangChainå¤„ç†RAGå¢å¼ºå¯¹è¯...'));

      // æ„å»ºRAGå¢å¼ºçš„æ¶ˆæ¯
      const ragPrompt = this.buildRAGPrompt(query, context);
      
      const input = {
        system_prompt: systemPrompt || 'ä½ æ˜¯ä¸€ä¸ªåŸºäºçŸ¥è¯†åº“çš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚',
        chat_history: this.convertToChatHistory(chatHistory),
        input: ragPrompt
      };

      console.log(chalk.blue('ğŸ“¤ RAGæŸ¥è¯¢:'), query);
      console.log(chalk.cyan('ğŸ“– ä¸Šä¸‹æ–‡é•¿åº¦:'), context.length, 'å­—ç¬¦');

      // è°ƒç”¨é“¾
      const response = await this.chain.invoke(input);

      // ä¼°ç®—tokenä½¿ç”¨é‡
      const promptTokens = this.estimateTokens(JSON.stringify(input));
      const completionTokens = this.estimateTokens(response);

      return {
        content: response,
        usage: {
          promptTokens,
          completionTokens,
          totalTokens: promptTokens + completionTokens
        }
      };

    } catch (error) {
      console.error(chalk.red('âŒ RAGå¢å¼ºè°ƒç”¨å¤±è´¥:'), error);
      throw error;
    }
  }

  private buildRAGPrompt(query: string, context: string): string {
    return `åŸºäºä»¥ä¸‹çŸ¥è¯†åº“ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

${context}

ç”¨æˆ·é—®é¢˜ï¼š${query}

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†åº“ä¿¡æ¯è¿›è¡Œå›ç­”ã€‚å¦‚æœçŸ¥è¯†åº“ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ èƒ½ç»™å‡ºçš„é€šç”¨å›ç­”ã€‚`;
  }

  // æµå¼å“åº”æ”¯æŒ
  public async *generateStreamResponse(
    messages: ChatMessage[], 
    systemPrompt?: string
  ): AsyncGenerator<string, void, unknown> {
    if (!this.model) {
      throw new Error('æ¨¡å‹æœªåˆå§‹åŒ–');
    }

    try {
      const chatHistory = this.convertToChatHistory(messages);
      
      if (systemPrompt) {
        chatHistory.unshift(new SystemMessage(systemPrompt));
      }

      const stream = await this.model.stream(chatHistory);

      for await (const chunk of stream) {
        if (chunk.content) {
          yield chunk.content.toString();
        }
      }
    } catch (error) {
      console.error(chalk.red('âŒ æµå¼å“åº”å¤±è´¥:'), error);
      throw error;
    }
  }
} 